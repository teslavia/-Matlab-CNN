# 基于MATLAB的CNN手写数字识别

[![MATLAB](https://img.shields.io/badge/MATLAB-R2020+-orange.svg)](https://www.mathworks.com/products/matlab.html)
[![License](https://img.shields.io/badge/license-Academic-blue.svg)](LICENSE)

> 电子科技大学（UESTC）深度学习导论课程期末作业
> 从零实现卷积神经网络，无深度学习框架依赖

## 📖 项目简介

本项目使用纯MATLAB实现了一个经典的卷积神经网络（CNN），用于MNIST手写数字识别任务。通过手动实现前向传播、反向传播算法，深入理解深度学习的底层原理。

**核心特点**：
- ✅ 无框架依赖，从零实现CNN所有组件
- ✅ 清晰的模块化设计，易于理解和修改
- ✅ 完整的前向传播和反向传播实现
- ✅ 支持MNIST标准数据集

## 🏗️ 网络架构

```
输入层 (28×28 灰度图像)
    ↓
卷积层 (20个 9×9 卷积核) → 输出: 20×20×20
    ↓
ReLU激活函数
    ↓
平均池化层 (2×2) → 输出: 10×10×20 = 2000维
    ↓
全连接层1 (2000 → 100)
    ↓
ReLU激活函数
    ↓
全连接层2 (100 → 10)
    ↓
Softmax分类器 → 输出: 10类概率分布
```

**参数统计**：
| 层 | 参数量 | 输出尺寸 |
|---|---|---|
| 卷积层 | 1,620 | 20×20×20 |
| 池化层 | 0 | 10×10×20 |
| 全连接1 | 200,000 | 100×1 |
| 全连接2 | 1,000 | 10×1 |
| **总计** | **202,620** | - |

## 📁 项目结构

```
-Matlab-/
│
├── CNN.m              # 主训练和测试脚本
├── Conv.m             # 卷积层前向传播
├── Pool.m             # 平均池化层
├── ReLU.m             # ReLU激活函数
├── Softmax.m          # Softmax分类器
├── MNISTData.mat      # MNIST数据集（需自行准备）
└── README.md          # 项目说明文档
```

## 🚀 快速开始

### 环境要求
- MATLAB R2016b 或更高版本
- 无需额外工具箱

### 数据准备

1. 下载MNIST数据集
2. 准备包含以下变量的 `MNISTData.mat` 文件：
   - `X_Train`: 训练图像 (28×28×60000)
   - `D_Train`: 训练标签 (10×60000, one-hot编码)
   - `X_Test`: 测试图像 (28×28×10000)
   - `D_Test`: 测试标签 (10×10000, one-hot编码)

### 运行训练

```matlab
% 在MATLAB命令窗口中运行
CNN
```

训练完成后会自动在测试集上评估，并输出准确率。

## 🔧 核心模块说明

### 1. 卷积层 (Conv.m)
```matlab
function OutputArg = Conv(W1, x)
    for k=1:20
        OutputArg(:,:, k) = filter2(W1(:,:,k), x, 'valid');
    end
end
```
- 使用MATLAB内置 `filter2` 函数
- `'valid'` 模式确保无边界填充
- 输出特征图尺寸: (28-9+1) × (28-9+1) = 20×20

### 2. 平均池化层 (Pool.m)
```matlab
function OutputArg = Pool(Y1)
    OutputArg = (Y1(1:2:end,1:2:end,:) + Y1(2:2:end,1:2:end,:)...
        + Y1(1:2:end, 2:2:end,:) + Y1(2:2:end,2:2:end,:))/4;
end
```
- 2×2窗口，步长为2
- 通过索引切片实现高效平均池化
- 特征图尺寸减半

### 3. 激活函数

**ReLU**: `y = max(0, x)`
**Softmax**: `y = exp(x) / sum(exp(x))`

### 4. 反向传播 (CNN.m)

**梯度计算流程**：
1. 输出层误差: `e = d - y`
2. 全连接层梯度: `δ₃ = (v₃ > 0) .* (W₄ᵀδ)`
3. 池化层梯度: 将梯度均匀分配到4个位置
4. 卷积层梯度: 使用卷积运算计算权重梯度

**权重更新**：
```matlab
W = W + α * δ * xᵀ  % α = 0.01
```

## 📊 超参数配置

| 参数 | 值 | 说明 |
|---|---|---|
| 学习率 | 0.01 | 固定学习率 |
| Epoch数 | 2 | 训练轮数 |
| 批大小 | 1 | 随机梯度下降（SGD） |
| 优化器 | SGD | 无动量 |

## ⚠️ 已知问题

### 技术问题
1. **卷积梯度计算** (CNN.m:60)
   - 当前实现可能不符合严格的卷积梯度公式
   - 建议使用相关运算或转置卷积

2. **Softmax数值稳定性** (Softmax.m:5)
   - 大数值可能导致 `exp` 溢出
   - 建议: `exp(x - max(x))`

3. **计时逻辑错误** (CNN.m:23-24)
   - `tic` 后立即 `toc` 导致t3≈0
   - 应将 `tic` 移至训练循环前

### 性能限制
- 仅2个epoch，可能欠拟合
- 无验证集监控
- 无学习率衰减
- 逐样本训练效率低

## 🎯 性能指标

**预期准确率**：
- 理论范围: 85-92%
- LeNet-5基准: 98-99%
- 简单MLP: 92-95%

**训练时间**：
- 单个epoch: ~3-5分钟（CPU）
- 总训练时间: ~6-10分钟

## 🚧 改进方向

### 优先级高
- [ ] 修复卷积梯度计算
- [ ] 添加Softmax数值稳定性处理
- [ ] 增加训练轮数至10-20
- [ ] 实现Mini-batch训练

### 优先级中
- [ ] 添加验证集和早停机制
- [ ] 实现学习率衰减
- [ ] 使用Xavier/He权重初始化
- [ ] 添加训练过程可视化

### 优先级低
- [ ] 支持Dropout正则化
- [ ] 支持Batch Normalization
- [ ] 实现Adam优化器
- [ ] 模型保存和加载功能

## 📚 技术原理

### 卷积操作
卷积层通过共享权重的局部感受野提取特征：
```
y[i,j,k] = Σ Σ w[m,n,k] * x[i+m, j+n]
           m n
```

### 反向传播核心公式
```
∂L/∂W = δ * aᵀ        (全连接层)
∂L/∂W = Conv(δ, x)    (卷积层)
δₗ = (Wₗ₊₁ᵀ δₗ₊₁) ⊙ σ'(vₗ)
```

## 📖 参考资料

- [MNIST数据集](http://yann.lecun.com/exdb/mnist/)
- [LeNet-5论文](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
- [CS231n: Convolutional Neural Networks](http://cs231n.github.io/)

## 📄 许可证

本项目仅用于学术和教学目的。

## 👨‍💻 作者

UESTC 深度学习导论课程学生作品

---

**Note**: 本项目旨在教学演示，不建议用于生产环境。如需高性能实现，建议使用PyTorch或TensorFlow等成熟框架。
